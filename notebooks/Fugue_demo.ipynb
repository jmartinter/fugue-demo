{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee6e0dd9",
   "metadata": {},
   "source": [
    "# Fugue demo "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "143a917a",
   "metadata": {},
   "source": [
    "## What is fugue?\n",
    "\n",
    "Fugue is an open-source abstraction layer that was created to provide a seamless transition from a single machine to a distributed compute setting. \n",
    "\n",
    "With Fugue, users can code their logic in native Python, Pandas, or SQL, and Fugue will adapt it for execution in a Spark or Dask distributed engine."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57b2260b",
   "metadata": {},
   "source": [
    "## Fugue benefits\n",
    "\n",
    "* Cross-framework code\n",
    "* Rapid interactions\n",
    "* Friendly interface for Spark\n",
    "* Easily testable code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "129ccc4a",
   "metadata": {},
   "source": [
    "## Fugue drawbacks\n",
    "\n",
    "* Adds a new dependecy to your app\n",
    "* Distributed engines become a black box\n",
    "* Harder to debug\n",
    "* Easily testable code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d037f276",
   "metadata": {},
   "source": [
    "## A simple Fugue example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89dd6c2a",
   "metadata": {},
   "source": [
    "In Fugue, the construction of workflow and the construction of ExecutionEngines can be totally decoupled.\n",
    "\n",
    "A Fugue workflow is static, it is just the description of your logic flow, and it is independent from execution. When you finish defining the workflow, you can also choose an ExecutionEngine for the end to end execution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "966c6e68",
   "metadata": {},
   "source": [
    "### Generate some test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fdca9e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec0f85f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "random.seed(1234)\n",
    "\n",
    "def generate_hotels(num: int, iso2_codes: List[str]) -> pd.DataFrame:\n",
    "    \"\"\" Generates a hotel list with hotel id, country code and number of stars \"\"\"\n",
    "    data = {\n",
    "        \"cluster_id\": [f\"c_{i}\" for i in range(num)],\n",
    "        \"iso2_country\": [random.choice(iso2_codes) for i in range(num)],\n",
    "        \"stars\": [random.randint(1, 5) for i in range(num)]\n",
    "    }\n",
    "    return pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e6df027",
   "metadata": {},
   "outputs": [],
   "source": [
    "iso2_code_map = {\"ES\": \"Spain\", \"IT\": \"Italy\", \"FR\": \"France\", \"DE\": \"Germany\", \"AT\": \"Austria\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec483eeb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num = 40\n",
    "hotels_df = generate_hotels(num, list(iso2_code_map.keys()))\n",
    "hotels_df.sort_values(by=[\"iso2_country\", \"stars\"]).head(num)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5eca36e",
   "metadata": {},
   "source": [
    "### Define the business logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa5d4d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#schema: cluster_id: str, iso2_country: str, stars: int\n",
    "def filter_by_countries_py(data: List[Dict[str, Any]], countries: List[str]) -> List[Dict[str, Any]]:\n",
    "    return [row for row in data if row[\"iso2_country\"] in countries]\n",
    "\n",
    "# schema: cluster_id: str, iso2_country: str, stars: int, country: str\n",
    "def find_country_name_py(data: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n",
    "    for row in data:\n",
    "        row['country'] = iso2_code_map.get(row[\"iso2_country\"])\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06144699",
   "metadata": {},
   "source": [
    "### And the workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e8fd0c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fugue import FugueWorkflow\n",
    "\n",
    "countries = [\"AT\", \"DE\"]\n",
    "\n",
    "dag = FugueWorkflow()\n",
    "df = dag.df(hotels_df.copy())\n",
    "filtered_df = df.process(filter_by_countries_py, params={\"countries\": countries})\n",
    "enriched_df = filtered_df.process(find_country_name_py)\n",
    "enriched_df.show(num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5abe88e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dag.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1237db9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fugue import FugueWorkflow\n",
    "\n",
    "countries = [\"AT\", \"DE\"]\n",
    "\n",
    "with FugueWorkflow() as dag:\n",
    "    df = dag.df(hotels_df.copy())\n",
    "    filtered_df = df.process(filter_by_countries_py, params={\"countries\": countries})\n",
    "    enriched_df = filtered_df.process(find_country_name_py)\n",
    "    enriched_df.show(num)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d42c07b4",
   "metadata": {},
   "source": [
    "### Now execute the workflow in a Spark cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7330ea75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pyspark.sql\n",
    "from pyspark.sql import functions as F, types as T\n",
    "\n",
    "ENV_PATH = \"hdfs:///user/javiers/fugue_demo/support/fugue_conda_env.tar.gz#cluster_venv\"\n",
    "\n",
    "os.environ[\"PYSPARK_PYTHON\"] = \"./cluster_venv/bin/python\"\n",
    "\n",
    "spark = pyspark.sql.SparkSession \\\n",
    "    .builder.master('yarn') \\\n",
    "    .appName('demo') \\\n",
    "    .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\\n",
    "    .config(\"spark.sql.hive.metastore.version\", \"1.2.1\") \\\n",
    "    .config(\"spark.sql.hive.metastore.jars\", \"/opt/hive-1.2-jars/*\") \\\n",
    "    .config(\"spark.executor.instances\", '2') \\\n",
    "    .config(\"spark.executor.cores\", '1') \\\n",
    "    .config(\"spark.yarn.dist.archives\", ENV_PATH) \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()\n",
    "    \n",
    "    #.config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\") \\\n",
    "    \n",
    "spark.sparkContext.setLogLevel(\"WARN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a697f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fugue import FugueWorkflow\n",
    "from fugue_spark import SparkExecutionEngine\n",
    "\n",
    "countries = [\"AT\", \"DE\"]\n",
    "\n",
    "engine = SparkExecutionEngine(spark)\n",
    "\n",
    "with FugueWorkflow(engine) as dag:\n",
    "    df = dag.df(hotels_df.copy())\n",
    "    filtered_df = df.process(filter_by_countries_py, params={\"countries\": countries})\n",
    "    enriched_df = filtered_df.process(find_country_name_py)\n",
    "    enriched_df.show(num)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdf47ff2",
   "metadata": {},
   "source": [
    "### An alternative implementation using pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebcbbc1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_by_countries(df: pd.DataFrame, countries: List[str]) -> pd.DataFrame:\n",
    "    \"\"\" Filter hotels in given country code list \"\"\"\n",
    "    return df[df.iso2_country.isin(countries)]\n",
    "\n",
    "def find_country_name(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\" Set the country name associated to hotel country code \"\"\"\n",
    "    df['country'] = df.iso2_country.apply(lambda code: iso2_code_map.get(code))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b11fe3a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fugue import FugueWorkflow\n",
    "from fugue_spark import SparkExecutionEngine\n",
    "\n",
    "countries = [\"AT\", \"DE\"]\n",
    "\n",
    "engine = SparkExecutionEngine(spark)\n",
    "\n",
    "with FugueWorkflow(engine) as dag:\n",
    "    df = dag.df(hotels_df.copy())\n",
    "    filtered_df = df.process(filter_by_countries, params={\"countries\": countries})\n",
    "    enriched_df = filtered_df.process(find_country_name)\n",
    "    enriched_df.show(num)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf459c58",
   "metadata": {},
   "source": [
    "## Main syntax options"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2d6ed79",
   "metadata": {},
   "source": [
    "### Method 1: Native approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba0809a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fugue import FugueWorkflow\n",
    "\n",
    "\n",
    "def find_country_name_py(data: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n",
    "    for row in data:\n",
    "        row['country'] = iso2_code_map.get(row[\"iso2_country\"])\n",
    "    return data\n",
    "\n",
    "\n",
    "with FugueWorkflow() as dag:\n",
    "    df = dag.df(hotels_df.copy())\n",
    "    enriched_df = df.process(\n",
    "        find_country_name_py, schema=\"cluster_id: str, iso2_country: str, stars: int, country: str\"\n",
    "    )\n",
    "    enriched_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0eb6c39",
   "metadata": {},
   "source": [
    "### Method 2: Schema Hint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7edde1f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fugue import FugueWorkflow\n",
    "\n",
    "\n",
    "# schema: cluster_id: str, iso2_country: str, stars: int, country: str\n",
    "def find_country_name_py(data: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n",
    "    for row in data:\n",
    "        row['country'] = iso2_code_map.get(row[\"iso2_country\"])\n",
    "    return data\n",
    "\n",
    "\n",
    "with FugueWorkflow() as dag:\n",
    "    df = dag.df(hotels_df.copy())\n",
    "    enriched_df = df.process(find_country_name_py)\n",
    "    enriched_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a42dcce",
   "metadata": {},
   "source": [
    "### Method 3: Decorator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32fc6c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fugue import processor\n",
    "\n",
    "\n",
    "@processor(\"cluster_id: str, iso2_country: str, stars: int, country: str\")\n",
    "def find_country_name_py(data: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n",
    "    for row in data:\n",
    "        row['country'] = iso2_code_map.get(row[\"iso2_country\"])\n",
    "    return data\n",
    "\n",
    "\n",
    "with FugueWorkflow() as dag:\n",
    "    df = dag.df(hotels_df.copy())\n",
    "    enriched_df = df.process(find_country_name_py)\n",
    "    enriched_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dc4157f",
   "metadata": {},
   "source": [
    "## A more complex workflow using built-in operators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8a90a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "hotels_path = \"hdfs:///user/javiers/fugue_demo/data/hotels.csv\"\n",
    "hotels_schema = \"cluster_id: str, iso2_country: str, stars: int, trust_score: int\"\n",
    "\n",
    "prices_path = \"hdfs:///user/javiers/fugue_demo/data/prices.csv\"\n",
    "prices_schema = \"cluster_id: str, avg_price: int\"\n",
    "\n",
    "output_path = \"hdfs:///user/javiers/fugue_demo/output/at_de_hotels_with_prices.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d20310b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_by_countries(df: pd.DataFrame, countries: List[str]) -> pd.DataFrame:\n",
    "    return df[df.iso2_country.isin(countries)]\n",
    "\n",
    "\n",
    "def find_country_name(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df['country'] = df.iso2_country.apply(lambda x: iso2_code_map.get(x))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "921d6023",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fugue import FugueWorkflow\n",
    "from fugue_spark import SparkExecutionEngine\n",
    "\n",
    "\n",
    "engine = SparkExecutionEngine(spark)\n",
    "\n",
    "with FugueWorkflow(engine) as dag:\n",
    "    \n",
    "    my_hotels_df = dag.load(hotels_path, header=True, columns=hotels_schema)\n",
    "    \n",
    "    my_hotels_df = my_hotels_df.process(filter_by_countries, params={\"countries\": [\"AT\", \"DE\"]})\n",
    "    my_hotels_df = my_hotels_df.process(find_country_name)\n",
    "    \n",
    "    prices_df = dag.load(prices_path, header=True, columns=prices_schema)\n",
    "    \n",
    "    my_hotels_with_price_df = my_hotels_df \\\n",
    "        .left_outer_join(prices_df, on=[\"cluster_id\"])\n",
    "    \n",
    "    final_df = my_hotels_with_price_df \\\n",
    "        .rename({\"cluster_id\": \"hotel_id\"}) \\\n",
    "        .drop([\"iso2_country\"]) \\\n",
    "        .persist()\n",
    "    \n",
    "    final_df[[\"hotel_id\", \"country\", \"avg_price\"]] \\\n",
    "        .save(output_path, mode=\"overwrite\", header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8421873d",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.read.csv(output_path, header=True, \n",
    "               schema=\"`cluster_id` STRING, `country` STRING, `avg_price` INT\"\n",
    "              ).show(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "430e0b6f",
   "metadata": {},
   "source": [
    "Note that `load` method only allows CSV, JSON and PARQUET formats!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dd4080b",
   "metadata": {},
   "source": [
    "## Extensions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a60620e",
   "metadata": {},
   "source": [
    "There are only 3 types of nodes in Fugue workflow. They are also called driver side extensions.\n",
    "\n",
    "<img src=\"../images/nodes.svg\" width=\"500\">\n",
    "\n",
    "* **Creator**: no input, outputs a single output dataframe\n",
    "* **Processor**: one or multiple input dataframes, outputs a single output dataframe\n",
    "* **Outputter**: one or multiple input dataframes, no output\n",
    "\n",
    "All these nodes/extensions work on the whole dataset and they are **ExecutionEngine aware**.\n",
    "\n",
    "There are two special types of Processors: **Transformer** and **CoTransformer**. They are special because they are **NOT ExecutionEngine aware**, and they work on partition level\n",
    "\n",
    "<img src=\"../images/transformers.svg\" width=\"300\">\n",
    "\n",
    "* **Transformer**: single dataframe in, single dataframe out\n",
    "* **CoTransformer**: one or multiple dataframes in, single dataframe out\n",
    "\n",
    "The inputs and outputs for the extensions have to be Fugue DataFrames.\n",
    "\n",
    "We orchestrate the extensions to generate a Fugue Workflow."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ef1a60",
   "metadata": {},
   "source": [
    "### Creator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edc9ebc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date\n",
    "from dateutil.rrule import rrule, DAILY\n",
    "from typing import List, Any\n",
    "\n",
    "from fugue import FugueWorkflow\n",
    "\n",
    "\n",
    "#schema: a:str\n",
    "def create_dates(start: date, end: date) -> List[List[Any]]:\n",
    "    return [ [dt.strftime(\"%Y-%m-%d\")] for dt in rrule(DAILY, dtstart=start, until=end)]\n",
    "\n",
    "\n",
    "with FugueWorkflow() as dag:\n",
    "    start_date = date(2021, 9, 1)\n",
    "    end_date = date(2021, 9, 15)\n",
    "    \n",
    "    df = dag.create(create_dates, params={\"start\": start_date, \"end\": end_date})\n",
    "    df.show(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0514141c",
   "metadata": {},
   "source": [
    "#### Accessing the distributed engine API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "621305ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fugue import ExecutionEngine, DataFrame\n",
    "from fugue_spark import SparkExecutionEngine, SparkDataFrame\n",
    "\n",
    "\n",
    "def read_from_hive(e: ExecutionEngine, num_rows: int) -> DataFrame:\n",
    "    \n",
    "    assert isinstance(e, SparkExecutionEngine)\n",
    "    \n",
    "    query = f\"SELECT cluster_id, iso2_country FROM hotel4x.hotel LIMIT {num_rows}\"\n",
    "    \n",
    "    sdf= e.spark_session.sql(query)\n",
    "    return SparkDataFrame(sdf) # has to return a Fugue SparkDataFrame\n",
    "\n",
    "\n",
    "def read_orc(e: ExecutionEngine) -> DataFrame:\n",
    "    \n",
    "    assert isinstance(e, SparkExecutionEngine)\n",
    "    \n",
    "    path = \"fugue_demo/data/hotels.orc\"\n",
    "    \n",
    "    sdf= e.spark_session.read.orc(path).limit(5)\n",
    "    return SparkDataFrame(sdf) # has to return a Fugue SparkDataFrame\n",
    "\n",
    "\n",
    "engine = SparkExecutionEngine(spark)\n",
    "\n",
    "with FugueWorkflow(engine) as dag:\n",
    "    dag.create(read_from_hive, params={\"num_rows\":5}).show()\n",
    "    dag.create(read_orc).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "710a0327",
   "metadata": {},
   "source": [
    "### Processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "781b2609",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Iterable, Dict, Any, List\n",
    "from fugue import DataFrames, DataFrame\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def concat(dfs:DataFrames) -> pd.DataFrame:\n",
    "    pdfs = [df.as_pandas() for df in dfs.values()]\n",
    "    return pd.concat(pdfs).reset_index(drop=True)\n",
    "\n",
    "with FugueWorkflow() as dag:\n",
    "    df1 = dag.df([[0,1]], \"one:int, two:int\")\n",
    "    df2 = dag.df([[1,1],[1,2]], \"one:int, two:int\")\n",
    "    df3 = dag.df([[2,1], [2,2], [2,3]], \"one:int, two:int\")\n",
    "    \n",
    "    df = dag.process(df1, df2, df3, using=concat)\n",
    "    df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a5ca07c",
   "metadata": {},
   "source": [
    "### Outputter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18aea613",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fugue import DataFrame\n",
    "import pandas as pd\n",
    "\n",
    "def save_unique(df1: DataFrame, df2: DataFrame, path: str) -> None:\n",
    "    unique_df = pd.concat([df1.as_pandas(), df2.as_pandas()]) \\\n",
    "        .reset_index(drop=True) \\\n",
    "        .drop_duplicates() \\\n",
    "        .to_csv(path, header=True, index=False)\n",
    "\n",
    "path = \"../output/outputter_example.csv\"    \n",
    "\n",
    "with FugueWorkflow() as dag:\n",
    "    df1 = dag.df([[1,1], [2,3]], \"one:int, two:int\")\n",
    "    df2 = dag.df([[1,1],[1,2]], \"one:int, two:int\")\n",
    "    \n",
    "    dag.output(df1, df2, using=save_unique, params={\"path\": path})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1b460ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat ../output/outputter_example.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14aa7c29",
   "metadata": {},
   "source": [
    "### Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00462a0d",
   "metadata": {},
   "source": [
    "Transformer represents the logic unit executing on logical partitions of the input dataframe. The partitioning logic is not a concern of Transformer, it should be specified in a previous step.\n",
    "\n",
    "Transformer and Co-Transformer require users to be explicit on the output schema. \n",
    "\n",
    "To make it easier, `*` can represent the input dataframe schema, so `*, b:int` means the output will have an additional column `b`. Only transformers we can use this special [syntax](https://triad.readthedocs.io/en/latest/api/triad.collections.html#triad.collections.schema.Schema.transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb9a11f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#schema: *-cluster_id, +c: int\n",
    "def aggregate_by_country_and_stars(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    agg_df = df.groupby([\"iso2_country\", \"stars\"]).count().rename(columns={\"cluster_id\": \"c\"})\n",
    "    agg_df.reset_index(inplace=True)\n",
    "    return agg_df.sort_values([\"iso2_country\", \"stars\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0a258c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fugue import FugueWorkflow\n",
    "\n",
    "\n",
    "with FugueWorkflow() as dag:\n",
    "    df = dag.df(hotels_df.copy())\n",
    "    df = df.transform(aggregate_by_country_and_stars)\n",
    "    df.show(num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f1767cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fugue import FugueWorkflow\n",
    "from fugue_spark import SparkExecutionEngine\n",
    "\n",
    "\n",
    "engine = SparkExecutionEngine(spark)\n",
    "\n",
    "with FugueWorkflow(engine) as dag:\n",
    "    df = dag.df(hotels_df.copy())\n",
    "    df = df.transform(aggregate_by_country_and_stars)\n",
    "    df.show(num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "261a539a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fugue import FugueWorkflow\n",
    "from fugue_spark import SparkExecutionEngine\n",
    "\n",
    "engine = SparkExecutionEngine(spark)\n",
    "\n",
    "with FugueWorkflow(engine) as dag:\n",
    "    df = dag.df(hotels_df.copy())\n",
    "    df = df.partition(by=[\"iso2_country\"]).transform(aggregate_by_country_and_stars)\n",
    "    df.show(num)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0197f1cb",
   "metadata": {},
   "source": [
    "### Co-transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07ffd908",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "\n",
    "\n",
    "cities = [\"Munich\", \"Cluj\", \"Madrid\"]\n",
    "start_date = datetime(2021, 5, 1)\n",
    "\n",
    "def generate_temp(num: int, base_temp: 0) -> pd.DataFrame:\n",
    "    \"\"\" Generate a dataframe with random temperatures for different cities and dates \"\"\"\n",
    "\n",
    "    data = {\n",
    "        \"city\": [random.choice(cities) for i in range(num)],\n",
    "        \"date\": [(start_date + timedelta(days=i)).strftime(\"%Y-%m-%d\") for i in range(num)],\n",
    "        \"temp\": [base_temp + random.randint(-5, 5) for i in range(num)]\n",
    "    }\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "\n",
    "# schema: city: str, avg_min: float, avg_max: float \n",
    "def calculate_avg_temps(min_temp_df: pd.DataFrame, max_temp_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \n",
    "    avg_min_temp_df = min_temp_df \\\n",
    "        .groupby(\"city\").mean() \\\n",
    "        .rename(columns={\"temp\": \"avg_min\"}) \\\n",
    "        .reset_index(level=0)\n",
    "    avg_max_temp_df = max_temp_df \\\n",
    "        .groupby(\"city\").mean() \\\n",
    "        .rename(columns={\"temp\": \"avg_max\"}) \\\n",
    "        .reset_index(level=0)\n",
    "    \n",
    "    return avg_min_temp_df.merge(avg_max_temp_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7560bcc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fugue import FugueWorkflow\n",
    "\n",
    "with FugueWorkflow() as dag:\n",
    "    min_temp_df = dag.create(generate_temp, params={\"num\": 25, \"base_temp\": 4})\n",
    "    max_temp_df = dag.create(generate_temp, params={\"num\": 25, \"base_temp\": 15})\n",
    "    \n",
    "    # Both dataframes have to be equally partitioned\n",
    "    final_df = min_temp_df.zip(\n",
    "        max_temp_df, \n",
    "        how=\"inner\", \n",
    "        partition={\"by\": [\"city\"]}\n",
    "    ).transform(calculate_avg_temps)\n",
    "    \n",
    "    final_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45d01eb6",
   "metadata": {},
   "source": [
    "## FugueSQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd6fe4f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fugue_sql import FugueSQLWorkflow\n",
    "\n",
    "def find_country_name(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df['country'] = df.iso2_country.apply(lambda code: iso2_code_map.get(code))\n",
    "    return df\n",
    "\n",
    "\n",
    "with FugueSQLWorkflow() as dag:\n",
    "    df = dag.df(hotels_df.copy())\n",
    "    \n",
    "    country_code = \"AT\"\n",
    "    output_path = \"/usr/local/trustyou/home/javiers/sandbox/fugue/output/at_hotels_from_sql.csv\"\n",
    "    \n",
    "    dag(\"\"\"\n",
    "        SELECT * FROM df WHERE iso2_country='{{country_code}}'\n",
    "        TRANSFORM USING find_country_name() SCHEMA *, country: str\n",
    "        PRINT\n",
    "        SAVE OVERWRITE SINGLE CSV \"{{output_path}}\"\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5181f770",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat ../output/at_hotels_from_sql.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04f559db",
   "metadata": {},
   "source": [
    "### FugueSQL notebook extension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8ae0667",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fugue_notebook import setup\n",
    "setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d913dcf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "country_code = \"DE\"\n",
    "output_path = \"/usr/local/trustyou/home/javiers/sandbox/fugue/output/de_hotels_from_sql.csv\"\n",
    "\n",
    "df = hotels_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7165470f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%fsql\n",
    "\n",
    "SELECT * FROM df WHERE iso2_country='{{country_code}}'\n",
    "TRANSFORM USING find_country_name() SCHEMA *, country: str\n",
    "PRINT\n",
    "SAVE OVERWRITE SINGLE CSV \"{{output_path}}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "278bccfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat ../output/de_hotels_from_sql.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c604a98",
   "metadata": {},
   "source": [
    "## Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9170cec9",
   "metadata": {},
   "source": [
    "### Official resources\n",
    "* [Documentation](https://fugue.readthedocs.io/en/latest/)\n",
    "* [Tutorials](https://fugue-project.github.io/tutorials/index.html)\n",
    "* [Code repository](http://github.com/fugue-project/fugue)\n",
    "\n",
    "### Articles\n",
    "* https://jameskle.com/writes/fugue\n",
    "* https://databricks.com/session_na20/fugue-unifying-spark-and-non-spark-ecosystems-for-big-data-analytics\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
